\documentclass[a4paper,12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{subcaption}
\usepackage{setspace}
\usepackage{minted}
\usepackage{authblk}
\usepackage{caption}
\captionsetup[listing]{labelformat=empty}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{regexpatch}
\usepackage{dsfont} 
\usepackage{mathrsfs}
\usepackage{array}
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{trees}
\usetikzlibrary{positioning}
\usepackage{xcolor}
\usepackage{pgfkeys}
\usepackage{booktabs}
\usepackage{listings}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage[noend]{algpseudocode}
\usepackage{fancyhdr}
\setlength{\parindent}{0pt}
\geometry{a4paper, margin=2.5cm}

\sloppy
\begin{document}

\begin{titlepage}

    \vspace*{4cm}

    \centering
    
    \includegraphics[width=0.6\textwidth]{Images/logo.png} \\[1.5cm]
    
    \rule{\linewidth}{1pt} \\[1cm]

    {\Huge \bfseries HAX907X - Apprentissage statistique}\\[0.5cm]
    {\Huge TP : Support Vector Machines (SVM)}\\[1cm]
    
    \rule{\linewidth}{1pt} \\[2cm]

    {\Large \textbf{Marine GERMAIN}}\\
    {\Large \textbf{Coralie ROMANI DE VINCI}}\\[1cm]
    

\end{titlepage}


\renewcommand{\contentsname}{Table des matières}
\tableofcontents


\newpage

\section{Base de données Iris}

Dans cette section, nous étudierons la base de données Iris sur laquelle nous ferons une étude de classification sur les classes $1$ et $2$.
Nous utiliserons d'abord le noyau linéaire puis le noyau polynomial et nous les comparerons.
Nous avons séparé le dataset en deux parties : un ensemble d'entraînement (75\% des données) et un ensemble de test(25\% des données). 

\subsection{Classification avec noyau linéaire}

La classification des deux premières variables avec un noyau linéaire a pour objectif de trouver un hyperplan qui sépare au mieux les deux classes en maximisant la marge entre les points les plus proches de chaque classe. 
Les scores obtenus pour cette méthode sont les suivants :

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/linear_score.png}
    \caption{Score obtenu pour le noyau linéaire}
    \label{fig:linear}
\end{figure}

Pour les données d'entraînement, le modèle a classifié correctement 69,3\% des données, contre 64\% pour les données de test. 


\subsection{Classification avec noyau polynomial}

La classification des deux premières variables avec un noyau polynomial a pour objectif de trouver une frontière de décision non linéaire capable de séparer au mieux les deux classes. 
Nous souhaitons ici voir si l'emploi d'un noyau polynomial nous permet d'obtenir un meilleur classifieur que celui obtenu précédemment.\\
Les scores obtenus pour cette méthode sont les suivants :

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Images/poly_score.png}
    \caption{Score obtenu pour le noyau polynomial}
    \label{fig:poly}
\end{figure}

Pour les données d'entraînement et de test, le modèle a classifié correctement 68\% des données. \\

\textbf{Comparaison des deux méthodes :}\\[0.5cm]
- Nous disposons d'abord des scores obtenus en figure \ref{fig:linear} et figure \ref{fig:poly}.
On remarque que le score pour le noyau linéaire est très légèrement plus élevé que celui du noyau polynomial pour les données d'entraînements $0.693$ contre $0.68$.
En revanche il est légèrement plus faible pour les données de test $0.64$ contre $0.68$. A noter que ces différences sont faibles et que ces deux méthodes semblent donc similaires.\\

- Ensuite, nous avons tracé graphiquement ces résultats à l'aide des frontières : \\
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/linear_vs_poly.png}
    \caption{Visualisation de la frontière séparant les classes 1 et 2 pour les noyaux linéaire et polynomial}
    \label{fig:compare}
\end{figure}

La figure \ref{fig:compare} permet de visualiser les deux classifieurs sur l'ensemble des données du dataset Iris.\\
On observe que la frontière du noyau polynomial semble plutôt linéaire.
La différence remarquable est que pour le noyau linéaire on observe qu'il y a moins de données "bleues" mal classifiées que pour le noyau polynomial.
On peut faire l'observation inverse concernant les données "oranges".
A REFORMULER + CONCLURE 

\section{SVM GUI}

Nous utilisons dans cette section le script \texttt{svm\_gui.py}.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/C=1.png}
        \caption{C = 1}
        \label{}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/C=0.7.png}
        \caption{C = 0.7}
        \label{}
    \end{subfigure}

    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/C=0.4.png}
        \caption{C = 0.4}
        \label{}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/C=0.3.png}
        \caption{C = 0.3}
        \label{}
    \end{subfigure}

    \caption{Visualisation de la région de décision selon différents paramètres de régularisation C}
    \label{fig:carre}
\end{figure}

CONCLURE \\

\newpage

\section{Classification des visages}

Dans cette section, nous exploitons une base d'images extraites de « Labeled Faces in the Wild ».
L'objectif est de classifier, à l'aide d'un SVM à noyau linéaire, deux types d'images : des portraits de Tony Blair (figure \ref{fig : visages}) et de Colin Powell. 


\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Images/visages.png}
    \caption{Base de données de 12 portraits de Tony Blair}
    \label{fig : visages}
\end{figure}

\subsection{Influence du paramètre de régularisation}

Nous souhaitons observer l'influence du paramètre de régularisation $C$ sur la qualité du classifieur. 
Pour cela, nous estimons le score d'apprentissage en fonction de ce paramètre en figure \ref{fig : C}.

\begin{figure}[H]
        \centering
        \includegraphics[width=0.5\textwidth]{Images/erreur_prediction.png}
        \caption{Score d'apprentissage en fonction du paramètres de régularisation C pour des classifieurs SVM à noyau linéaire}
        \label{fig : C}
\end{figure}

En faisant varier les valeurs de C entre $10^{-5}$ et $10^{5}$, on remarque qu'on atteint un score d'apprentissage maximal d'environ $0.725$ lorsque $C=10^{-1}$.
Au delà de cette valeur de C, le score reste maximal de façon constante.
Ce résultat est cohérent, car il est attendu qu’un SVM atteigne un score maximal pour une valeur intermédiaire de 
C. En effet, si C est trop petit il y a un risque un sous-apprentissage et s'il est trop grand, il y a un risque de sur-apprentissage.
Ainsi, le paramètre de régularisation optimal est $C=0.1$.


\begin{figure}[H]    
        \centering    
        \includegraphics[width=0.6\textwidth]{Images/label_prediction.png}
        \caption{Visualisation de la prédiction, du test et de la précision}
        \label{fig:precision}
\end{figure}

Sur la figure \ref{fig:precision}, on observe d'abord la prédiction et le test associé.
Nous obtenons une excellente précision de la prédiction (88.9\%) avec le classifieur optimal. 
On a, en figure \ref{fig:predict}, un échantillon des résultats obtenues : sur 12 images, 11 ont été correctement prédites.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Images/prediction_visage.png}
    \caption{Prédictions faites à partir du classifieur optimal sur un échantillon d'images}
    \label{ref:predict}
\end{figure}

\subsection{Variable de nuisances}

Montrons que le score de prédiction est sensible au bruit. Pour cela, nous ajoutons des variables de bruit et
recommençons la prédiction. Le bruitage consiste à ajouter des variables de bruit (ici 300), et à les mélanger aux autres. 

Nous obtenons alors les valeurs suivantes avec et sans nuisances :

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/avec_variable.png}
    \caption{Score avec variable de nuisance}
    \label{fig: avec}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/sans_variable.png}
    \caption{Score sans variable de nuisance}
    \label{fig: sans}
\end{figure}

Nous constatons que le bruit endommage fortement le classifieur qui voit son score presque réduit de moitié.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Images/coeff_visage.png}
    \caption{WTF}
    \label{fig:?}
\end{figure}

\subsection{Réduction de dimensions}

A présent, nous voulons améliorer la prédiction obtenue dans la question précédente à l'aide d'une réduction de
dimension.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/apres_reduction.png}
    \caption{}
    \label{fig:dim}
\end{figure}



\end{document}